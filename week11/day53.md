# 5) BERT 언어모델 기반의 단일 문장 분류
---

## KLUE 데이터셋 소개

한국어 자연어 이해 벤치마크(Korean Language Understanding Evaluation, KLUE)

* 살면서 직면하게 되는 거의 모든 자연어 task 유형

**단일 문장 분류 task**

* 문장 분류

* 관계 추출

**문장 임베딩 벡터의 유사도(e.g. [CLS])**

* 문장 유사도

**두 문장 관계 분류 task**

* 자연어 추론

**문장 토큰 분류 task**

* 개체명 인식

* 품사 태깅

* 질의 응답

**DST 강의**

* 목적형 대화

**단어간 관계 분석**

* 의존 구문 분석

## 의존 구문 분석

단어들 사이의 관계를 분석하는 task

1. 특징

* 지배소: 의미의 중심이 되는 요소

* 의존소: 지배소가 갖는 의미를 보완해주는 요소 (수식)

* 어순과 생략이 자유로운 한국어와 같은 언어에서 주로 연구된다.

2. 분류 규칙

* 지배소는 후위언어이다. 즉 지배소는 항상 의존소보다 뒤에 위치한다.

* 각 의존소의 지배소는 하나이다.

* 교차 의존 구조는 없다.

3. 분류 방법

* Sequence labeling 방식으로 처리 단계를 나눈다.

* 앞 어절에 의존소가 없고 다음 어절이 지배소인 어절을 삭제하며 의존 관계를 만든다.

### 활용

복잡한 자연어 형태를 그래프로 구조화해서 표현 가능, 각 대상에 대한 정보 추출이 가능

문장: 내가 그린 구름그림은 새털구름 그린 구름그림이다.

분류

구름그림 -> 새털구름을 그린 것 -> 내가 그린 것

* "나"는 "구름그림"을 그렸다.

* "구름 그림"은 "새털구름"을 그린 것이다.

## 단일 문장 분류 task

주어진 문장이 어떤 종류의 범주에 속하는지를 구분하는 task

1. 감정분석(Sentiment Analysis)

문장의 긍정, 부정, 중립 등 성향을 분류하는 프로세스

**활용방안**

* 혐오 발언 분류

* 기업 모니터링

<br/>

2. 주제 라벨링(Topic Labeling)

문장의 내용을 이해하고 적절한 범주를 분류하는 프로세스

**활용방안**

* 대용량 문서 분류: 대용량의 문서를 범주화

* VoC(Voice of Customer): 고객의 피드백을 적절한 주제로 분류하여 데이터 구조화

<br/>

3. 언어 감지(Language Detection)

문장이 어떤 나라 언어인지 분류하는 프로세스

**활용방안**

* 번역기

* 데이터 필터링

<br/>

4. 의도 분류(Intent Classification)

문장이 가진 의도를 분류하는 프로세스

**활용방안**

* 챗봇: 문장의 의도인 질문, 명령, 거절 등을 분석하고 적절한 답변을 주기 위해 활용

<br/>

## 문장 분류를 위한 데이터

### Kor_hate

* 혐오 표현에 대한 데이터

* 특정 개인 또는 집단에 대한 공격적 문장

* 무례, 공격적이거나 비꼬는 문장

* 부정적이지 않은 문장

### Kor_sarcasm

* 비꼬지 않은 표현의 문장

* 비꼬는 표현의 문장

### Kor_sae

* 예/아니오로 답변 가능한 질문

* 대안 선택을 묻는 질문

* Wh- 질문

* 금지 명령

* 요구 명령

* 강한 요구 명령

### Kor_3i4k

* 단어 또는 문장 조각

* 평서문

* 질문

* 수사적 질문

* 수사적 명령문

* 억양에 의존하는 의도

<br/>

## 단일 문장 분류 모델 학습

### 모델 구조도

BERT의 [CLS] token의 vector를 classification 하는 Dense layer 사용

### 주요 매개변수

Input_ids: sequence token을 입력

attention_mask: [0,1]로 구성된 마스크이며 패딩 토큰을 구분

token_type_ids: [0,1]로 구성되었으며 입력의 첫 문장과 두번째 문장 구분

position_ids: 각 입력 시퀀스의 임베딩 인덱스

inputs_embeds: input_ids 대신 직접 임베딩 표현을 할당

labels: loss 계산을 위한 레이블

Next_sentence_label: 다음 문장 예측 loss 계산을 위한 레이블

## 학습 과정

1. Dataset 다운로드

2. Dataset 전처리 및 토큰화

3. Dataloader 설계

4. Train, Test Dataset 준비

5. Training Arguments 설정

6. Pretrained Model import

7. Trainer 설정

8. Model 학습

9. Predict 함수 구현 및 평가

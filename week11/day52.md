# 4) BERT Pre-Training
---

## BERT 모델 학습

**BERT 학습의 단계**

1. Tokenizer 만들기

2. 데이터셋 확보

3. Next sentence prediction (NSP)

4. Masking

**BERT 학습 이유**

도메인 특화 task의 경우, 도메인 특화된 학습 데이터만 사용하는 것이 성능이 더 좋다!

## 실습